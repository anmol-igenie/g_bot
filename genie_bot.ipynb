{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Env Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack import Document\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "from haystack.utils.auth import Secret\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack.components.embedders import OpenAITextEmbedder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from io import BytesIO\n",
    "\n",
    "# Initialize the BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\"))\n",
    "\n",
    "# Get a client for the specific container\n",
    "container_client = blob_service_client.get_container_client(\"prod-container\")\n",
    "\n",
    "# Specify the exact path of the folder\n",
    "genie_product='brand_pulse'\n",
    "client_name='kenvue'\n",
    "product_category='US_skincare'\n",
    "folder_path = f\"outputs/{genie_product}/{client_name}/{product_category}/bot_output/\"\n",
    "\n",
    "# List blobs in the exact folder\n",
    "blob_list = container_client.list_blobs(name_starts_with=folder_path)\n",
    "# Initialize a list to store documents\n",
    "documents = []\n",
    "dfs = []\n",
    "\n",
    "for blob in blob_list:\n",
    "    blob_client = container_client.get_blob_client(blob)\n",
    "    blob_extension = blob.name.split('.')[-1].lower()\n",
    "\n",
    "    # Download and read the blob content based on file type\n",
    "    if blob_extension == 'csv':\n",
    "        blob_content = blob_client.download_blob().readall()\n",
    "        df = pd.read_csv(BytesIO(blob_content))\n",
    "    elif blob_extension == 'parquet':\n",
    "        blob_content = blob_client.download_blob().readall()\n",
    "        df = pd.read_parquet(BytesIO(blob_content))\n",
    "    else:\n",
    "        continue  # Skip if it's not a CSV or Parquet file\n",
    "\n",
    "    # Convert the entire DataFrame to a JSON string\n",
    "    documents.append({\n",
    "        'content': df.to_json(orient='records'),  # Converting the DataFrame to JSON with records orientation\n",
    "        'name': blob.name\n",
    "    })\n",
    "\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert your documents into Haystack Documents and write them to the store\n",
    "haystack_documents = [Document(content=doc['content'], meta={\"name\": doc['name']}) for doc in documents]\n",
    "len(haystack_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x76579cca9c70>\n",
       "ðŸš… Components\n",
       "  - splitter: DocumentSplitter\n",
       "  - embedder: OpenAIDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - splitter.documents -> embedder.documents (List[Document])\n",
       "  - embedder.documents -> writer.documents (List[Document])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = DocumentSplitter(split_length = 200, split_overlap = 10, split_threshold = 20)\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
    "writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"splitter\", splitter)\n",
    "indexing.add_component(\"embedder\", embedder)\n",
    "indexing.add_component(\"writer\", writer)\n",
    "\n",
    "indexing.connect(\"splitter\", \"embedder\")\n",
    "indexing.connect(\"embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:58<00:00,  1.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embedder': {'meta': {'model': 'text-embedding-3-small',\n",
       "   'usage': {'prompt_tokens': 3924550, 'total_tokens': 3924550}}},\n",
       " 'writer': {'documents_written': 1939}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing.run({ \"splitter\": {\"documents\": haystack_documents } })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import json\n",
    "\n",
    "file_path = \"field_description.yaml\"\n",
    "yaml_file_path = os.path.join(os.getcwd(), file_path)\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    field_data = yaml.safe_load(file)\n",
    "\n",
    "df_name = \"summary\"\n",
    "table_description_dict = field_data.get(f\"{df_name}\").get(\"table_description\", \" \")\n",
    "field_descriptions_dict = field_data.get(f\"{df_name}\").get(\"field_description\", \" \")\n",
    "\n",
    "table_description = json.dumps(table_description_dict)\n",
    "field_descriptions = json.dumps(field_descriptions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are an 25+ year experienced `Advanced Business Analyst & counsellor` with statistical capability and domain expertise.\n",
    "Your primary task is to assist non-technical users in analyzing data with deep dive along with inference and suggestions as needed.\n",
    "Strictly, Don't assume random data, in any case.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Length Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component\n",
    "from pandasai.llm import OpenAI\n",
    "from pandasai import Agent\n",
    "from response_parser import GenieResponse\n",
    "from pandasai.connectors import PandasConnector\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class LengthValidator:\n",
    "    def __init__(self, max_length=1000000, max_tokens=100000):\n",
    "        self.max_length = max_length\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    @component.output_types(prompt=List[ChatMessage])\n",
    "    def run(self, prompt:List[ChatMessage], **kwargs):\n",
    "        \"\"\"\n",
    "        Checks if the content exceeds the max length. If so, trims the content from the end.\n",
    "        \"\"\"\n",
    "        # print(prompt)\n",
    "        content_str = prompt[0].content\n",
    "        \n",
    "        if len(content_str) > self.max_length:\n",
    "            # Trimming from the end\n",
    "            content_str = content_str[:self.max_length]\n",
    "            print(f\"Content was too long, trimmed to {self.max_length} characters.\")\n",
    "\n",
    "        if len(content_str.split()) > self.max_tokens:\n",
    "            # Trimming from the end\n",
    "            content_str = ' '.join(content_str.split()[:self.max_tokens])\n",
    "            print(f\"Content was too long, trimmed to {self.max_length} characters.\")\n",
    "\n",
    "        # Update the content back into the dictionary\n",
    "        prompt[0].content = content_str\n",
    "        \n",
    "        return {\"prompt\": prompt}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = \"\"\"\n",
    "Answer Strictly only for the asked original question in format of CRISP YET INSIGHTFUL Enterprise Grade Business Case Study ( dont't mention).\n",
    "Include focused analysis for all possible nuances in the asked question.\n",
    "If the answer is not contained within the context, reply with clarification questions. Don't assume random data, in any case.\n",
    "\n",
    "```\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "   {{ doc.content }}\n",
    "{% endfor %}\n",
    "```\n",
    "\n",
    "```\n",
    "Table descriptions:\n",
    "{{table_description}}\n",
    "\n",
    "Fields/Columns description:\n",
    "{{field_descriptions}}\n",
    "```\n",
    "Instructions:\n",
    "1. Respond insighfully in crisp, structured and bulleted formats with facts, numbers (strictly up to 2 decimal only) and inferences.\n",
    "2. Strictly Avoid mentioning phrases like \"Based on the shared context/dataset\" etc.\n",
    "3. When need to provide analysis, Use quantitative values to provide factual accuracy instead of relying upon the text mentions.\n",
    "\n",
    "Question: {{ query }}\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = \"\"\"Given the conversation history and the provided supporting documents, give a brief answer to the question.\n",
    "Note that supporting documents are not part of the conversation. If question can't be answered from supporting documents, say so & if needed ask clarification questions.\n",
    "\n",
    "Answer Strictly only for the asked original question in format of CRISP YET INSIGHTFUL Enterprise Grade Business Case Study ( dont't mention).\n",
    "Include focused analysis for all possible nuances in the asked question.\n",
    "If the answer is not contained within the context, reply with clarification questions. Don't assume random data.\n",
    "\n",
    "    Conversation history:\n",
    "    {% for memory in memories %}\n",
    "        {{ memory.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    Supporting documents:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc }}\n",
    "    {% endfor %}\n",
    "\n",
    "Instructions:\n",
    "1. Respond insighfully as a \"Pro Business Analyst\" in well-structured and bulleted formats with facts, numbers (strictly up to 2 decimal only) and drawn inferences (TLdr).\n",
    "2. Strictly Avoid mentioning phrases like \"Based on the shared context/dataset\", \"mentioned\", \"shared\" etc.\n",
    "3. When need to provide analysis, Use quantitative values to provide factual accuracy instead of relying upon the text mentions.\n",
    "\n",
    "\\nQuestion: {{query}}\n",
    "\\nAnswer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component, Document\n",
    "import concurrent.futures\n",
    "from typing import List\n",
    "from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "@component\n",
    "class BatchGenerator:\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def __init__(self, question_prompt, system_prompt, table_description, field_descriptions, batch_size: int = 10):\n",
    "        self.question_prompt = question_prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.batch_size = batch_size\n",
    "        self.table_description=table_description\n",
    "        self.field_descriptions=field_descriptions\n",
    "\n",
    "    @component.output_types(replies=List)\n",
    "    def run(self, documents: List[Document], user_asked_query: str, **kwargs):\n",
    "\n",
    "        self.user_asked_query = user_asked_query\n",
    "        batches_of_10 = [documents[i:i + self.batch_size] for i in range(0, len(documents), self.batch_size)]\n",
    "        print(f\"Total Batch Size {len(batches_of_10)} from {len(documents)} documents\")\n",
    "        # Process batches in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            batch_results = list(executor.map(self.process_batch, batches_of_10))\n",
    "\n",
    "        # Combine all batch results into one final output\n",
    "        # final_result = \"\\n\".join(batch_results)\n",
    "        # print(final_result)\n",
    "        return {\"replies\": batch_results}\n",
    "\n",
    "    def process_batch(self, batch_doc: List[Document]):\n",
    "\n",
    "        # prompt_builder = PromptBuilder(template=self.question_prompt)\n",
    "        prompt_builder = ChatPromptBuilder(template=[ChatMessage.from_user(self.question_prompt)])\n",
    "        generator = OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0, 'seed':101, 'n':1, 'max_tokens':1000 })\n",
    "\n",
    "        batch_rag = Pipeline()\n",
    "        batch_rag.add_component(\"prompt\", prompt_builder)\n",
    "        batch_rag.add_component(\"length_val\", LengthValidator())\n",
    "        batch_rag.add_component(\"generator\", generator)\n",
    "        batch_rag.connect(\"prompt\", \"length_val\")\n",
    "        batch_rag.connect(\"length_val\", \"generator\")\n",
    "        # Generate output for individual batch\n",
    "        response = batch_rag.run({\"prompt\": {\"documents\": batch_doc\n",
    "                                             , \"query\": self.user_asked_query\n",
    "                                             , \"table_description\":self.table_description\n",
    "                                             , \"field_descriptions\":self.field_descriptions\n",
    "                                }})\n",
    "\n",
    "        return response[\"generator\"][\"replies\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DATAFRAME CONTEXT Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class DataContextGenerator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm_model = \"gpt-4o-mini\"\n",
    "        self.temperature = 0.2\n",
    "        self.memory_size = 25\n",
    "\n",
    "    def load_yaml(self, file_path):\n",
    "        \"\"\"Load YAML file synchronously.\"\"\"\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return yaml.safe_load(file)\n",
    "\n",
    "    def run_chat(self, prompt, agent):\n",
    "        \"\"\"Run the agent and handle DataFrame conversion to JSON.\"\"\"\n",
    "        try:\n",
    "            result = agent.chat(prompt, output_type='dataframe')\n",
    "\n",
    "            if isinstance(result, str):\n",
    "                return result\n",
    "            else:\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return \"\"\n",
    "\n",
    "    def run_agent(self, question, df):\n",
    "        # Load the YAML file (assumes it's in the current directory)\n",
    "        file_path = \"field_description.yaml\"\n",
    "        yaml_file_path = os.path.join(os.getcwd(), file_path)\n",
    "        field_data = self.load_yaml(yaml_file_path)\n",
    "\n",
    "        df_name = \"summary\"\n",
    "\n",
    "        connector = PandasConnector({\"original_df\": df}\n",
    "                            , name=f\"{df_name}\"\n",
    "                            , description=field_data.get(f\"{df_name}\").get(\"table_description\", \" \")\n",
    "                            , field_descriptions=field_data.get(f\"{df_name}\").get(\"field_description\", \" \"))\n",
    "\n",
    "        # Initialize LLM\n",
    "        llm = OpenAI(model=self.llm_model, temperature=self.temperature, seed=10)\n",
    "\n",
    "        # Create the agent\n",
    "        agent = Agent(\n",
    "            connector,\n",
    "            memory_size=self.memory_size,\n",
    "            description=\"\"\"You are a 25+ year experienced `Pro Python & Pandas Assistant`. You specialize in developing precise dataframe generation code.\n",
    "                Your primary task is to assist non-technical users by providing relevant dataframe to start the analysis, by aggregating at all relevant granularity.\n",
    "                If the answer is not contained within the documents, reply with 'no_data'.\n",
    "                By default consider Latest few `Year_Month` data, unless asked specifically otherwise, and mention it.\n",
    "\n",
    "                ```\n",
    "                Instructions:\n",
    "                1. STRICTLY AVOID CHARTS, REVERT ONLY DATAFRAME.\n",
    "                2. ENSURE TO DO SANITY CHECKS & HANDLE ERRORS INTERNALLY TO ANSWER.\n",
    "                3. WHEN FETCHING HIGHEST, STRICTLY CONSIDER ALL MATCHING ITEMS WITH SIMILAR VALUES TO INCLUDE IN RESPONSE.\n",
    "                ```\n",
    "                \"\"\",\n",
    "            config={\n",
    "                \"llm\": llm,\n",
    "                \"open_charts\": False,\n",
    "                \"save_charts\": False,\n",
    "                \"verbose\": False,\n",
    "                \"save_logs\": False,\n",
    "                \"response_parser\": GenieResponse,\n",
    "                \"max_retries\": 2\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Rephrase query and ask clarification questions\n",
    "        rephrased_prompt = agent.rephrase_query(question)\n",
    "        # clarification_questions = agent.clarification_questions(str(question))\n",
    "\n",
    "        # Run the initial prompt and rephrased prompt concurrently\n",
    "        response_dict = dict()\n",
    "\n",
    "        response_dict[question], response_dict[rephrased_prompt] = self.run_chat(question, agent), self.run_chat(rephrased_prompt, agent)\n",
    "\n",
    "        # Run clarification questions concurrently\n",
    "        # clarifications_responses = [self.run_chat(que, agent) for que in clarification_questions]\n",
    "\n",
    "        # for que, resp in zip(clarification_questions, clarifications_responses):\n",
    "        #     response_dict[que] = resp\n",
    "\n",
    "        return response_dict\n",
    "\n",
    "    @component.output_types(replies=str)\n",
    "    def run(self, user_asked_query: str, df_list: List, **kwargs):\n",
    "        data_context_dict = self.run_agent(user_asked_query, df_list[0])\n",
    "        return {\"replies\": json.dumps(data_context_dict)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "@component\n",
    "class ContextWindowLimiter:\n",
    "\n",
    "    def trim_context(self, paragraph: str, max_tokens=100000):\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Assuming GPT-4 tokenizer\n",
    "        tokens = tokenizer.encode(paragraph)\n",
    "\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return paragraph\n",
    "\n",
    "        trimmed_tokens = tokens[-max_tokens:]\n",
    "        trimmed_paragraph = tokenizer.decode(trimmed_tokens)\n",
    "\n",
    "        return trimmed_paragraph\n",
    "\n",
    "    def count_tokens(self, paragraph: str):\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(tokenizer.encode(paragraph))\n",
    "\n",
    "    @component.output_types(replies=str)\n",
    "    def run(self, paragraph: str, **kwargs):\n",
    "        limited_paragraph = self.trim_context(paragraph)\n",
    "        return {\"replies\": limited_paragraph}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_context_prompt = \"\"\"Given the conversation history and the provided supporting documents, give a brief answer to the question.\n",
    "Note that supporting documents are not part of the conversation. If question can't be answered from supporting documents, say so & if needed ask clarification questions.\n",
    "\n",
    "Answer Strictly only for the asked original question in format of Enterprise Grade Business Case Study ( dont't mention).\n",
    "Include focused analysis for all possible nuances in the asked question.\n",
    "Here are the relevant dataframes extracted for the user asked questions and some clarification questions as well as context.\n",
    "\n",
    "    Conversation history:\n",
    "    {% for memory in memories %}\n",
    "        {{ memory.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    Supporting documents:\n",
    "    {{documents}}\n",
    "\n",
    "Instructions:\n",
    "1. Have facts, numbers (strictly up to 2 decimal only) and drawn inferences (TLdr).\n",
    "2. Strictly Avoid mentioning phrases like \"Based on the shared context/dataset\", \"mentioned\", \"shared\" etc.\n",
    "3. For numeric values, ensure to provide data in efficient PIVOT form tabular grids, if more than 2 rows, as per relevance and provide insights from it in reference to nuances of questions asked.\n",
    "4. When analyse, prioritize only latest of months / years data, unless asked specifically otherwise.\n",
    "5. Ensure to provide an enterprise grade answer in sections, but only limit to asked question.\n",
    "\n",
    "\\nQuestion: {{query}}\n",
    "\\nAnswer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pipeline Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_prompt = \"\"\"Based on the conversation history, user question, and the table structure with column descriptions, decide which ONE of the following should be used for the next step of analysis:\n",
    "\n",
    "1. **RAG Pipeline**: Choose this if:\n",
    "   - The question involves analyzing textual data embedded in the table (e.g., free-text fields, long descriptions).\n",
    "   - The user question involves a broad analysis that isn't about performing calculations or operations on specific columns of the table.\n",
    "\n",
    "2. **DATAFRAME Pipeline**: Choose this if:\n",
    "   - The question directly/indirectly may require numerical columns or operations or date month year column\n",
    "   - The user is asking for a straightforward statistical or numerical operation that can be executed directly on the dataset (e.g., sorting, filtering, grouping by a column).\n",
    "\n",
    "3. **Generic Response**: Choose this ONLY if:\n",
    "   - The user input is clearly a greeting, a generic comment, or irrelevant text that does not require any analytical pipeline.\n",
    "\n",
    "### Decision Rules:\n",
    "- If the question involves broad or interpretive insights, choose the `RAG Pipeline`.\n",
    "- If the question involves quantified analysis on data (e.g., sums, averages, comparisons, trends, date / month / year), choose the `DATAFRAME Pipeline`.\n",
    "- When both text and numerical elements are involved, select the pipeline that aligns best with the core requirement of the question (e.g., is the focus more on numeric calculations or text-based insights?).\n",
    "- **Prioritize the `DATAFRAME Pipeline`**: Use the `DATAFRAME Pipeline` for any question that involves quantified analysis or structured data, even if the question includes some general context. Only choose the `RAG Pipeline` if the core of the question clearly requires textual analysis.\n",
    "- **Limit `RAG Pipeline`** to situations where structured numerical analysis cannot solve the problem, such as when dealing with broad natural language queries or complex text-based analysis.\n",
    "- **Fallback to the `DATAFRAME Pipeline`** if there is ambiguity between numerical and textual analysis.\n",
    "- **Fallback to `Generic Response`** for anything irrelevant, greetings, or broad queries unrelated to the table data and including external calculations, not relevant to data.\n",
    "\n",
    "### Table Structure and Column Descriptions:\n",
    "    ```\n",
    "    Table descriptions:\n",
    "    {{table_description}}\n",
    "\n",
    "    Fields/Columns description:\n",
    "    {{field_descriptions}}\n",
    "    ```\n",
    "\n",
    "Carefully analyze the conversation history and the user question. Choose ONLY ONE option based on the best analysis of the input.\n",
    "\n",
    "    Conversation history:\n",
    "    {% for memory in memories %}\n",
    "        {{ memory.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    User Question:\n",
    "    {{query}}\n",
    "\n",
    "Return only one: `RAG Pipeline`, `DATAFRAME Pipeline`, or `GENERIC Response`.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = [\n",
    "    {\n",
    "        \"condition\": \"{{'rag' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_rag\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'dataframe' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_df\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'generic' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_generic\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'generic' not in replies[0]|lower}}\",\n",
    "        \"output\": \"{{replies[0]}}\",\n",
    "        \"output_name\": \"answer\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack import Pipeline\n",
    "# from haystack.dataclasses import ChatMessage\n",
    "\n",
    "# from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "# from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "# from haystack.components.routers import ConditionalRouter\n",
    "\n",
    "# router = ConditionalRouter(routes=routes)\n",
    "\n",
    "\n",
    "# demo_pipe = Pipeline()\n",
    "# demo_pipe.add_component(\"selector_prompt\", ChatPromptBuilder(template=[ChatMessage.from_user(selector_prompt)]))\n",
    "# demo_pipe.add_component(\"llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0, 'seed':101, 'n':1 }))\n",
    "# demo_pipe.add_component(\"router\", ConditionalRouter(routes=routes))\n",
    "\n",
    "# demo_pipe.connect(\"selector_prompt.prompt\", \"llm.messages\")\n",
    "# demo_pipe.connect(\"llm.replies\", \"router.replies\")\n",
    "\n",
    "# while True:\n",
    "#     question = input(\"Enter your question or Q to exit.\\nðŸ§‘ \")\n",
    "#     if question==\"Q\":\n",
    "#         break\n",
    "#     print(question)\n",
    "#     response = demo_pipe.run( data={\n",
    "#         \"selector_prompt\": {\"query\": question, \"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "#         \"router\": {\"query\": question},\n",
    "#         })\n",
    "#     print(response)\n",
    "#     # print(response[\"llm\"][\"replies\"][0].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generic Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_response_prompt = \"\"\"You are Genie Bot, an advanced and friendly analytical assistant developed by i-Genie.\n",
    "Given the conversation history, and mainly Based on the user's message, respond appropriately, whether it's a greeting, generic question, or analytical inquiry.\n",
    "Adjust your tone according to the user's sentiment, whether positive, neutral, or negative, while always maintaining professionalism.\n",
    "\n",
    "### Response Instructions:\n",
    "\n",
    "1. **If the user greets you**:\n",
    "   - Respond with enthusiasm: \"Hello! It's great to hear from you! ðŸ˜Š How can I assist you today with your data analysis?\"\n",
    "\n",
    "2. **If the user asks a generic or irrelevant question**:\n",
    "   - Attempt to answer based on general knowledge, but suggest the user ask more data-related questions for better insights.\n",
    "   - Politely guide the user with examples of business analysis questions, dynamically generated based on the provided data description.\n",
    "\n",
    "3. **If the user provides positive feedback**:\n",
    "   - Acknowledge their satisfaction: \"I'm glad that was helpful! ðŸ˜Š\"\n",
    "   - Then offer further analysis suggestions tailored to the data: \"Would you like to explore more detailed insights based on the current dataset?\"\n",
    "\n",
    "4. **If the user responds negatively**:\n",
    "   - Stay calm and acknowledge potential limitations: \"I understand this might not be what you expected, and Iâ€™m still learning. Sometimes, I may not be perfect.\"\n",
    "   - Encourage collaboration: \"Letâ€™s explore further together. Based on the dataset, you can ask specific questions that may help refine the analysis.\"\n",
    "   - Reassure them: \"Your feedback helps me improve, and we can work together to find the right insights.\"\n",
    "\n",
    "5. **If the user asks a data-related question**:\n",
    "   - Provide the analysis as requested, and then offer additional suggestions for further exploration, based on the dataset.\n",
    "\n",
    "6. **MUST suggest dynamic follow-up questions** based on the table and data description, without revealing schema details. Use the dataset to generate relevant, meaningful questions dynamically to keep the conversation engaging, but ensure that they are answerable from dataset.\n",
    "\n",
    "````\n",
    "### Underlying dataset Reference:\n",
    "Below is the general description of the dataset, which should guide your suggestions:\n",
    "    Table descriptions:\n",
    "    {{table_description}}\n",
    "\n",
    "    Fields/Columns description:\n",
    "    {{field_descriptions}}\n",
    "````\n",
    "\n",
    "    Conversation history:\n",
    "    {% for memory in memories %}\n",
    "        {{ memory.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "### Task:\n",
    "1. Judge the user's message and tone to respond accordingly, whether it's a greeting, irrelevant query, positive or negative feedback, or data-related question.\n",
    "2. Always suggest helpful business analysis question examples, crafted based on the data description & conversation history. You should be dynamically tailoring responses and follow-up questions based on the data description, coversation history and more importantly user question.\n",
    "3. While suggesting question examples, ensure they are not much complex yet insightful, and data oriented.\n",
    "4. Encourage deeper data exploration, keeping your tone friendly and professional, while guiding the user toward meaningful insights.\n",
    "\n",
    "Keep your tone friendly, professional, and energetic where appropriate! You are here to help the user make the most of the tool!\n",
    "\n",
    "\\n **User Message**: {{query}}\n",
    "\\n **Response**:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x765784312a50>\n",
       "ðŸš… Components\n",
       "  - s_prompt: ChatPromptBuilder\n",
       "  - s_lv: LengthValidator\n",
       "  - s_llm: OpenAIChatGenerator\n",
       "  - s_router: ConditionalRouter\n",
       "  - g_prompt: ChatPromptBuilder\n",
       "  - g_lv: LengthValidator\n",
       "  - g_llm: OpenAIChatGenerator\n",
       "  - dc_generator: DataContextGenerator\n",
       "  - dc_prompt_builder: ChatPromptBuilder\n",
       "  - dc_lv: LengthValidator\n",
       "  - dc_llm: OpenAIChatGenerator\n",
       "  - r_query_embedder: OpenAITextEmbedder\n",
       "  - r_retriever: InMemoryEmbeddingRetriever\n",
       "  - r_batch_generator: BatchGenerator\n",
       "  - r_prompt_builder: ChatPromptBuilder\n",
       "  - r_lv: LengthValidator\n",
       "  - r_llm: OpenAIChatGenerator\n",
       "  - memory_retriever: ChatMessageRetriever\n",
       "  - memory_writer: ChatMessageWriter\n",
       "  - memory_joiner: BranchJoiner\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - s_prompt.prompt -> s_lv.prompt (List[ChatMessage])\n",
       "  - s_lv.prompt -> s_llm.messages (List[ChatMessage])\n",
       "  - s_llm.replies -> s_router.replies (List[ChatMessage])\n",
       "  - s_router.go_to_rag -> r_query_embedder.text (str)\n",
       "  - s_router.go_to_df -> dc_generator.user_asked_query (str)\n",
       "  - s_router.go_to_generic -> g_prompt.query (str)\n",
       "  - g_prompt.prompt -> g_lv.prompt (List[ChatMessage])\n",
       "  - g_lv.prompt -> g_llm.messages (List[ChatMessage])\n",
       "  - g_llm.replies -> memory_joiner.value (List[ChatMessage])\n",
       "  - dc_generator.replies -> dc_prompt_builder.documents (str)\n",
       "  - dc_prompt_builder.prompt -> dc_lv.prompt (List[ChatMessage])\n",
       "  - dc_lv.prompt -> dc_llm.messages (List[ChatMessage])\n",
       "  - dc_llm.replies -> memory_joiner.value (List[ChatMessage])\n",
       "  - r_query_embedder.embedding -> r_retriever.query_embedding (List[float])\n",
       "  - r_retriever.documents -> r_batch_generator.documents (List[Document])\n",
       "  - r_batch_generator.replies -> r_prompt_builder.documents (List)\n",
       "  - r_prompt_builder.prompt -> r_lv.prompt (List[ChatMessage])\n",
       "  - r_lv.prompt -> r_llm.messages (List[ChatMessage])\n",
       "  - r_llm.replies -> memory_joiner.value (List[ChatMessage])\n",
       "  - memory_retriever.messages -> s_prompt.memories (List[ChatMessage])\n",
       "  - memory_retriever.messages -> dc_prompt_builder.memories (List[ChatMessage])\n",
       "  - memory_retriever.messages -> r_prompt_builder.memories (List[ChatMessage])\n",
       "  - memory_retriever.messages -> g_prompt.memories (List[ChatMessage])\n",
       "  - memory_joiner.value -> memory_writer.messages (List[ChatMessage])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.joiners import BranchJoiner\n",
    "from haystack.components.routers import ConditionalRouter\n",
    "\n",
    "from haystack_experimental.chat_message_stores.in_memory import InMemoryChatMessageStore\n",
    "from haystack_experimental.components.retrievers import ChatMessageRetriever\n",
    "from haystack_experimental.components.writers import ChatMessageWriter\n",
    "\n",
    "########################################\n",
    "##   CUSTOM COMPONENT INSTANTIATION   ##\n",
    "########################################\n",
    "\n",
    "# Memory Components Instantiation\n",
    "memory_store = InMemoryChatMessageStore()\n",
    "memory_retriever = ChatMessageRetriever(memory_store)\n",
    "memory_writer = ChatMessageWriter(memory_store)\n",
    "\n",
    "# Custom Components Instantiation\n",
    "batch_generator = BatchGenerator(question_prompt=map_prompt\n",
    "                                 , system_prompt=system_prompt\n",
    "                                 , table_description=table_description\n",
    "                                 , field_descriptions=field_descriptions\n",
    "                                 , batch_size=5)\n",
    "\n",
    "data_context_generator = DataContextGenerator()\n",
    "\n",
    "# Router Instantiation\n",
    "routes = [\n",
    "    {\n",
    "        \"condition\": \"{{'rag' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_rag\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'dataframe' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_df\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'generic' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_generic\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'generic' not in replies[0]|lower}}\",\n",
    "        \"output\": \"{{replies[0]}}\",\n",
    "        \"output_name\": \"answer\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "]\n",
    "\n",
    "########################################\n",
    "##     PIPELINE DEFINITION            ##\n",
    "########################################\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# components for Selected Routing\n",
    "pipeline.add_component(\"s_prompt\", ChatPromptBuilder(template=[ChatMessage.from_user(selector_prompt)]))\n",
    "pipeline.add_component(\"s_lv\", LengthValidator())\n",
    "pipeline.add_component(\"s_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0, 'seed':101, 'n':1, 'max_tokens':2000 }))\n",
    "pipeline.add_component(\"s_router\", ConditionalRouter(routes=routes))\n",
    "\n",
    "# components for Generic Response\n",
    "pipeline.add_component(\"g_prompt\", ChatPromptBuilder(template=[ChatMessage.from_user(generic_response_prompt)]))\n",
    "pipeline.add_component(\"g_lv\", LengthValidator())\n",
    "pipeline.add_component(\"g_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0.5, 'seed':101, 'n':1, 'max_tokens':2000 }))\n",
    "\n",
    "# components for DataFrame Context\n",
    "pipeline.add_component(\"dc_generator\", data_context_generator)\n",
    "pipeline.add_component(\"dc_prompt_builder\", ChatPromptBuilder(template=[ChatMessage.from_user(data_context_prompt)]))\n",
    "pipeline.add_component(\"dc_lv\", LengthValidator())\n",
    "pipeline.add_component(\"dc_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0.3, 'seed':101, 'n':1 , 'max_tokens':500}))\n",
    "\n",
    "# components for RAG\n",
    "pipeline.add_component(\"r_query_embedder\", OpenAITextEmbedder())\n",
    "pipeline.add_component(\"r_retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "pipeline.add_component(\"r_batch_generator\", batch_generator)\n",
    "pipeline.add_component(\"r_prompt_builder\", ChatPromptBuilder(template=[ChatMessage.from_user(reduce_prompt)]))\n",
    "pipeline.add_component(\"r_lv\", LengthValidator())\n",
    "pipeline.add_component(\"r_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0.3, 'seed':101, 'n':1, 'max_tokens':2000 }))\n",
    "\n",
    "# components for memory\n",
    "pipeline.add_component(\"memory_retriever\", memory_retriever)\n",
    "pipeline.add_component(\"memory_writer\", memory_writer)\n",
    "pipeline.add_component(\"memory_joiner\", BranchJoiner(List[ChatMessage]))\n",
    "\n",
    "# connections for Selected routing\n",
    "pipeline.connect(\"s_prompt\", \"s_lv\")\n",
    "pipeline.connect(\"s_lv.prompt\", \"s_llm.messages\")\n",
    "pipeline.connect(\"s_llm.replies\", \"s_router.replies\")\n",
    "pipeline.connect(\"s_router.go_to_rag\", \"r_query_embedder.text\")\n",
    "pipeline.connect(\"s_router.go_to_df\", \"dc_generator.user_asked_query\")\n",
    "pipeline.connect(\"s_router.go_to_generic\", \"g_prompt.query\")\n",
    "\n",
    "# connection for Generic Response\n",
    "pipeline.connect(\"g_prompt\", \"g_lv\")\n",
    "pipeline.connect(\"g_lv.prompt\", \"g_llm.messages\")\n",
    "pipeline.connect(\"g_llm.replies\", \"memory_joiner\")\n",
    "\n",
    "# connections for RAG\n",
    "pipeline.connect(\"r_query_embedder.embedding\", \"r_retriever.query_embedding\")\n",
    "pipeline.connect(\"r_retriever.documents\", \"r_batch_generator.documents\")\n",
    "pipeline.connect(\"r_batch_generator.replies\", \"r_prompt_builder.documents\")\n",
    "pipeline.connect(\"r_prompt_builder\", \"r_lv\")\n",
    "pipeline.connect(\"r_lv.prompt\", \"r_llm.messages\")\n",
    "pipeline.connect(\"r_llm.replies\", \"memory_joiner\")\n",
    "\n",
    "# connections for DataFrame Context\n",
    "pipeline.connect(\"dc_generator.replies\", \"dc_prompt_builder.documents\")\n",
    "pipeline.connect(\"dc_prompt_builder\", \"dc_lv\")\n",
    "pipeline.connect(\"dc_lv.prompt\", \"dc_llm.messages\")\n",
    "pipeline.connect(\"dc_llm.replies\", \"memory_joiner\")\n",
    "\n",
    "# connections for memory\n",
    "pipeline.connect(\"memory_joiner\", \"memory_writer\")\n",
    "pipeline.connect(\"memory_retriever\", \"s_prompt.memories\")\n",
    "pipeline.connect(\"memory_retriever\", \"dc_prompt_builder.memories\")\n",
    "pipeline.connect(\"memory_retriever\", \"r_prompt_builder.memories\")\n",
    "pipeline.connect(\"memory_retriever\", \"g_prompt.memories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversational Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_message = ChatMessage.from_system(system_prompt)\n",
    "\n",
    "# while True:\n",
    "#     messages = [system_message,]\n",
    "#     question = input(\"Enter your question or Q to exit.\\nðŸ§‘ \")\n",
    "#     if question==\"Q\":\n",
    "#         break\n",
    "\n",
    "#     res = pipeline.run(data={\"s_prompt\": {\"query\": question, \"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "#                              \"g_prompt\": {\"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "#                              \"s_router\": {\"query\": question},\n",
    "#                              \"dc_generator\": {\"df_list\":[df,]},\n",
    "#                              \"dc_prompt_builder\": {\"query\": question},\n",
    "#                              \"r_retriever\": {\"top_k\": 50},\n",
    "#                              \"r_batch_generator\": {\"user_asked_query\": question,},\n",
    "#                              \"r_prompt_builder\": {\"query\": question},\n",
    "#                              \"memory_joiner\": {\"value\": [ChatMessage.from_user(question)]}\n",
    "#                              },\n",
    "#                             include_outputs_from=['g_llm', 'dc_llm', 'r_llm', \"s_prompt\", \"s_router\",\n",
    "#                                                   \"dc_generator\", \"dc_prompt_builder\",\n",
    "#                                                   \"r_retriever\", \"r_batch_generator\", \"r_prompt_builder\", \"memory_retriever\"  ]\n",
    "#                         )\n",
    "\n",
    "#     llm_type = [key for key in ['g_llm', 'dc_llm', 'r_llm'] if key in res][0]\n",
    "#     assistant_resp = res[llm_type]['replies'][0]\n",
    "#     display(Markdown(f\"{assistant_resp.content}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res[\"memory_retriever\"][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradio Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ChatMessage.from_system(system_prompt)]\n",
    "\n",
    "def chat(question, history):\n",
    "    messages.append(ChatMessage.from_user(question))\n",
    "    response = pipeline.run(data={\"s_prompt\": {\"query\": question, \"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "                             \"g_prompt\": {\"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "                             \"s_router\": {\"query\": question},\n",
    "                             \"dc_generator\": {\"df_list\":[df,]},\n",
    "                             \"dc_prompt_builder\": {\"query\": question},\n",
    "                             \"r_retriever\": {\"top_k\": 50},\n",
    "                             \"r_batch_generator\": {\"user_asked_query\": question,},\n",
    "                             \"r_prompt_builder\": {\"query\": question},\n",
    "                             \"memory_joiner\": {\"value\": [ChatMessage.from_user(question)]}\n",
    "                             },\n",
    "                            include_outputs_from=['g_llm', 'dc_llm', 'r_llm', \"s_prompt\", \"s_router\",\n",
    "                                                  \"dc_generator\", \"dc_prompt_builder\",\n",
    "                                                  \"r_retriever\", \"r_batch_generator\", \"r_prompt_builder\",\n",
    "                                                   \"memory_retriever\" ]\n",
    "                        )\n",
    "    llm_type = [key for key in ['g_llm', 'dc_llm', 'r_llm'] if key in response][0]\n",
    "    assistant_resp = response[llm_type]['replies']\n",
    "    messages.extend(assistant_resp)\n",
    "\n",
    "    return assistant_resp[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache from '/workspaces/g_bot/gradio_cached_examples/15' directory. If method or examples have changed since last caching, delete this folder to clear cache.\n",
      "\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://93b0cbeeb1d167ac58.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://93b0cbeeb1d167ac58.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"What is the overall score?\",\n",
    "        \"How are my brands performing?\",\n",
    "    ],\n",
    "    title=\"Brand Pulse Bot\",\n",
    "    cache_examples=True,\n",
    "    analytics_enabled=True,\n",
    "    show_progress='full',\n",
    "    fill_height=True,\n",
    "\n",
    ")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
