{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports & Env Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(levelname)s - %(name)s -  %(message)s\", level=logging.WARNING)\n",
    "logging.getLogger(\"haystack\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Markdown, display\n",
    "import gradio as gr\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import Pipeline\n",
    "from haystack import Document\n",
    "from haystack.dataclasses import ChatMessage\n",
    "\n",
    "from haystack.utils.auth import Secret\n",
    "from haystack.components.builders import PromptBuilder\n",
    "\n",
    "from haystack.components.preprocessors.document_splitter import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.embedders import OpenAIDocumentEmbedder\n",
    "from haystack.components.embedders import OpenAITextEmbedder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Indexing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_store = InMemoryDocumentStore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "from io import BytesIO\n",
    "\n",
    "# Initialize the BlobServiceClient\n",
    "blob_service_client = BlobServiceClient.from_connection_string(os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\"))\n",
    "\n",
    "# Get a client for the specific container\n",
    "container_client = blob_service_client.get_container_client(\"prod-container\")\n",
    "\n",
    "# Specify the exact path of the folder\n",
    "genie_product='brand_pulse'\n",
    "client_name='kenvue'\n",
    "product_category='US_skincare'\n",
    "folder_path = f\"outputs/{genie_product}/{client_name}/{product_category}/bot_output/\"\n",
    "\n",
    "# List blobs in the exact folder\n",
    "blob_list = container_client.list_blobs(name_starts_with=folder_path)\n",
    "# Initialize a list to store documents\n",
    "documents = []\n",
    "dfs = []\n",
    "\n",
    "for blob in blob_list:\n",
    "    blob_client = container_client.get_blob_client(blob)\n",
    "    blob_extension = blob.name.split('.')[-1].lower()\n",
    "\n",
    "    # Download and read the blob content based on file type\n",
    "    if blob_extension == 'csv':\n",
    "        blob_content = blob_client.download_blob().readall()\n",
    "        df = pd.read_csv(BytesIO(blob_content))\n",
    "    elif blob_extension == 'parquet':\n",
    "        blob_content = blob_client.download_blob().readall()\n",
    "        df = pd.read_parquet(BytesIO(blob_content))\n",
    "    else:\n",
    "        continue  # Skip if it's not a CSV or Parquet file\n",
    "\n",
    "    # Convert the entire DataFrame to a JSON string\n",
    "    documents.append({\n",
    "        'content': df.to_json(orient='records'),  # Converting the DataFrame to JSON with records orientation\n",
    "        'name': blob.name\n",
    "    })\n",
    "\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert your documents into Haystack Documents and write them to the store\n",
    "haystack_documents = [Document(content=doc['content'], meta={\"name\": doc['name']}) for doc in documents]\n",
    "len(haystack_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x737f391e0e90>\n",
       "üöÖ Components\n",
       "  - splitter: DocumentSplitter\n",
       "  - embedder: OpenAIDocumentEmbedder\n",
       "  - writer: DocumentWriter\n",
       "üõ§Ô∏è Connections\n",
       "  - splitter.documents -> embedder.documents (List[Document])\n",
       "  - embedder.documents -> writer.documents (List[Document])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitter = DocumentSplitter(split_length = 200, split_overlap = 10, split_threshold = 20)\n",
    "embedder = OpenAIDocumentEmbedder(model=\"text-embedding-3-small\")\n",
    "writer = DocumentWriter(document_store=document_store)\n",
    "\n",
    "indexing = Pipeline()\n",
    "indexing.add_component(\"splitter\", splitter)\n",
    "indexing.add_component(\"embedder\", embedder)\n",
    "indexing.add_component(\"writer\", writer)\n",
    "\n",
    "indexing.connect(\"splitter\", \"embedder\")\n",
    "indexing.connect(\"embedder\", \"writer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 61/61 [01:24<00:00,  1.38s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'embedder': {'meta': {'model': 'text-embedding-3-small',\n",
       "   'usage': {'prompt_tokens': 3924550, 'total_tokens': 3924550}}},\n",
       " 'writer': {'documents_written': 1939}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexing.run({ \"splitter\": {\"documents\": haystack_documents } })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os\n",
    "import json\n",
    "\n",
    "file_path = \"field_description.yaml\"\n",
    "yaml_file_path = os.path.join(os.getcwd(), file_path)\n",
    "\n",
    "with open(file_path, \"r\") as file:\n",
    "    field_data = yaml.safe_load(file)\n",
    "\n",
    "df_name = \"summary\"\n",
    "table_description_dict = field_data.get(f\"{df_name}\").get(\"table_description\", \" \")\n",
    "field_descriptions_dict = field_data.get(f\"{df_name}\").get(\"field_description\", \" \")\n",
    "\n",
    "table_description = json.dumps(table_description_dict)\n",
    "field_descriptions = json.dumps(field_descriptions_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are a 25+ year experienced `Advanced Business Analyst & Counselor` with strong statistical expertise and deep domain knowledge.\n",
    "Your primary task is to assist non-technical users in analyzing data thoroughly, providing inferences and actionable suggestions as needed.\n",
    "Strictly avoid assuming or generating random data under any circumstances.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Length Validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component\n",
    "from pandasai.llm import OpenAI\n",
    "from pandasai import Agent\n",
    "from response_parser import GenieResponse\n",
    "from pandasai.connectors import PandasConnector\n",
    "\n",
    "import json\n",
    "import yaml\n",
    "import os\n",
    "from typing import List\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "@component\n",
    "class LengthValidator:\n",
    "    def __init__(self, max_length=1000000, max_tokens=100000):\n",
    "        self.max_length = max_length\n",
    "        self.max_tokens = max_tokens\n",
    "\n",
    "    @component.output_types(prompt=List[ChatMessage])\n",
    "    def run(self, prompt:List[ChatMessage], **kwargs):\n",
    "        \"\"\"\n",
    "        Checks if the content exceeds the max length. If so, trims the content from the end.\n",
    "        \"\"\"\n",
    "        # print(prompt)\n",
    "        content_str = str(prompt[0].content)\n",
    "        \n",
    "        if len(content_str) > self.max_length:\n",
    "            # Trimming from the end\n",
    "            content_str = content_str[-self.max_length:]\n",
    "            print(f\"Content was too long, trimmed to {self.max_length} characters.\")\n",
    "            \n",
    "        content_str = self.trim_context(content_str)\n",
    "\n",
    "        # Update the content back into the dictionary\n",
    "        prompt = [ChatMessage.from_user(content_str)]\n",
    "        \n",
    "        return {\"prompt\": prompt}\n",
    "    \n",
    "    def trim_context(self, paragraph: str, max_tokens=100000):\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # Assuming GPT-4 tokenizer\n",
    "        tokens = tokenizer.encode(paragraph)\n",
    "\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return paragraph\n",
    "\n",
    "        trimmed_tokens = tokens[-max_tokens:]\n",
    "        trimmed_paragraph = tokenizer.decode(trimmed_tokens)\n",
    "        print(f\"Content was too long, trimmed to {self.max_tokens} tokens.\")\n",
    "\n",
    "\n",
    "        return trimmed_paragraph\n",
    "\n",
    "    def count_tokens(self, paragraph: str):\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "        return len(tokenizer.encode(paragraph))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 RAG Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt = \"\"\"\n",
    "Context:\n",
    "{% for doc in documents %}\n",
    "   {{ doc.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Table descriptions:\n",
    "{{table_description}}\n",
    "\n",
    "Fields/Columns description:\n",
    "{{field_descriptions}}\n",
    "\n",
    "Answer strictly only for the asked original question in the format of a **CRISP YET INSIGHTFUL Enterprise Grade Business Case Study** (do not mention \"business case study\").  \n",
    "Ensure your response is succinct and directly to the point, including all parts of the question. Your answer should be as logical and factually accurate as possible, based on the retrieved documents from the RAG pipeline shared above as Context.  \n",
    "If the answer is not contained within the context, reply with clarification questions. Do not assume random data in any case.\n",
    "\n",
    "### Instructions:\n",
    "1. **Succinct and Insightful**: Respond in a concise, structured, and bulleted format. Include clear facts, numbers (up to 2 decimal places), and insights.\n",
    "2. **No Redundant Phrases**: Avoid phrases like \"Based on the shared context/dataset\" or similar.\n",
    "3. **Logical and Factual**: Address all parts of the question with logical reasoning and factual accuracy. Ensure you‚Äôre relying on quantitative data where possible.\n",
    "4. **Avoid Unnecessary Date References**: Only mention specific dates, months, or years when explicitly requested in the question. For date-related data, maintain a sequential order and include all available values without skipping.\n",
    "5. **Clarifications**: If the retrieved documents do not contain the answer, ask for clarifications instead of making assumptions.\n",
    "\n",
    "**Question:** {{ query }}\n",
    "\"\"\"\n",
    "\n",
    "reduce_prompt = enhanced_prompt = \"\"\"\n",
    "    Conversation history:\n",
    "    {% for memory in memories %}\n",
    "        {{ memory.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    LLM generated answers:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc }}\n",
    "    {% endfor %}\n",
    "\n",
    "Using the conversation history and LLM-generated answers, provide a **logical**, **insightful**, and **impactful** response to the question. Prioritize the retrieved documents as the main source of truth. If the question cannot be fully answered, respond with clarification questions. Avoid assuming any random data.\n",
    "\n",
    "Answer **strictly** for the asked question in the format of a **CRISP, INSIGHTFUL, and ENTERPRISE-GRADE Business Case Study** (do not mention \"business case study\"). Ensure the response is visually appealing, enriched with **dynamic emojis**, and addresses all parts of the query as logically and factually as possible.\n",
    "\n",
    "### Instructions:\n",
    "1. **Structured Response**: As a \"Pro Business Analyst\", present the answer in a **clear**, **well-structured**, and **visually enriched** format. Use bulleted lists, tables, and headers for clarity. Dynamically enrich the content with **emojis** to enhance the visual flow and emphasis.\n",
    "2. **Fact-Based Insights**: Provide facts, numbers (up to 2 decimal places), and inferences (in a TL;dr style). Use **quantitative values** for accuracy and insights.\n",
    "3. **Visual & Emoji Enhancement**:\n",
    "   - Use **tables** for clear presentation of key metrics.\n",
    "   - Use **bold**, **italics**, **headers**, and **relevant emojis** to emphasize important points.\n",
    "   - Ensure the response is **concise**, **engaging**, and **easy to follow**.\n",
    "4. **Avoid Redundancy**: Do not use phrases like \"Based on the shared context\" or \"mentioned.\"\n",
    "5. **Accurate Data Presentation**: Use quantitative data when providing analysis. \n",
    "6. If the answer cannot be fully derived from the documents, ask for **clarifications** rather than making assumptions.\n",
    "7. **Avoid Unnecessary Date References**: Only mention specific dates, months, or years when explicitly requested in the question. For date-related data, maintain a sequential order and include all available values without skipping.\n",
    "\n",
    "\n",
    "### üìä Key Metrics:\n",
    "| **Metric**       | **Value**    | **Insights**                        |\n",
    "|------------------|--------------|-------------------------------------|\n",
    "| **Revenue**      | **$1.75M**   | Increased by 12% over the last year üìà |\n",
    "| **Profit Margin**| **13.2%**    | Steady growth over the past 2 years üíπ |\n",
    "| **Expenses**     | **$0.9M**    | Reduced by 8%, optimizing operations üõ†Ô∏è |\n",
    "\n",
    "---\n",
    "\n",
    "### üîë **Key Insights**:\n",
    "- **Revenue** growth has been consistent over the past year, driven by improved product reach and market penetration üìà.\n",
    "- **Profit Margin** has maintained steady growth, with effective cost-cutting strategies improving operational efficiency üõ†Ô∏è.\n",
    "- **Expenses** have dropped significantly due to reduced operational costs, allowing for higher profit margins.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù **Actionable Steps**:\n",
    "1. **Expand into regions** with high growth potential to drive further revenue üìç.\n",
    "2. **Continue optimizing operational costs** to further improve margins üí°.\n",
    "3. **Invest in new product lines** to diversify income streams and reduce dependency on current markets üõí.\n",
    "\n",
    "---\n",
    "\n",
    "### üí° **Additional Insights & Next Steps**:\n",
    "- **Focus on key segments** with the highest profit potential.\n",
    "- **Monitor expense trends** to ensure continued optimization.\n",
    "- **Explore digital transformation** for enhanced operational efficiencies.\n",
    "\n",
    "**Question**: {{query}}\n",
    "**Answer**:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component, Document\n",
    "import concurrent.futures\n",
    "from typing import List\n",
    "from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.rankers import SentenceTransformersDiversityRanker\n",
    "\n",
    "@component\n",
    "class BatchGenerator:\n",
    "    outgoing_edges = 1\n",
    "\n",
    "    def __init__(self, question_prompt, system_prompt, table_description, field_descriptions, batch_size: int = 10):\n",
    "        self.question_prompt = question_prompt\n",
    "        self.system_prompt = system_prompt\n",
    "        self.batch_size = batch_size\n",
    "        self.table_description=table_description\n",
    "        self.field_descriptions=field_descriptions\n",
    "\n",
    "    @component.output_types(replies=List)\n",
    "    def run(self, documents: List[Document], user_asked_query: str, **kwargs):\n",
    "\n",
    "        self.user_asked_query = user_asked_query\n",
    "        batches_of_10 = [documents[i:i + self.batch_size] for i in range(0, len(documents), self.batch_size)]\n",
    "        print(f\"Total Batch Size {len(batches_of_10)} from {len(documents)} documents\")\n",
    "        # Process batches in parallel\n",
    "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            batch_results = list(executor.map(self.process_batch, batches_of_10))\n",
    "\n",
    "        # Combine all batch results into one final output\n",
    "        # final_result = \"\\n\".join(batch_results)\n",
    "        # print(final_result)\n",
    "        return {\"replies\": batch_results}\n",
    "\n",
    "    def process_batch(self, batch_doc: List[Document]):\n",
    "\n",
    "        # prompt_builder = PromptBuilder(template=self.question_prompt)\n",
    "        prompt_builder = ChatPromptBuilder(template=[ChatMessage.from_user(self.question_prompt)])\n",
    "        generator = OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0, 'seed':101, 'n':1, 'max_tokens':1000 })\n",
    "\n",
    "        batch_rag = Pipeline()\n",
    "        batch_rag.add_component(\"prompt\", prompt_builder)\n",
    "        batch_rag.add_component(\"length_val\", LengthValidator())\n",
    "        batch_rag.add_component(\"generator\", generator)\n",
    "        batch_rag.connect(\"prompt\", \"length_val\")\n",
    "        batch_rag.connect(\"length_val\", \"generator\")\n",
    "        # Generate output for individual batch\n",
    "        response = batch_rag.run({\"prompt\": {\"documents\": batch_doc\n",
    "                                             , \"query\": self.user_asked_query\n",
    "                                             , \"table_description\":self.table_description\n",
    "                                             , \"field_descriptions\":self.field_descriptions\n",
    "                                }})\n",
    "\n",
    "        return response[\"generator\"][\"replies\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 DATAFRAME CONTEXT Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@component\n",
    "class DataContextGenerator:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm_model = \"gpt-4o-mini\"\n",
    "        self.temperature = 0.2\n",
    "        self.memory_size = 25\n",
    "\n",
    "    def load_yaml(self, file_path):\n",
    "        \"\"\"Load YAML file synchronously.\"\"\"\n",
    "        with open(file_path, \"r\") as file:\n",
    "            return yaml.safe_load(file)\n",
    "\n",
    "    def run_chat(self, prompt, agent):\n",
    "        \"\"\"Run the agent and handle DataFrame conversion to JSON.\"\"\"\n",
    "        try:\n",
    "            result = agent.chat(prompt, output_type='dataframe')\n",
    "\n",
    "            if isinstance(result, str):\n",
    "                return result\n",
    "            else:\n",
    "                return \"\"\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    def run_agent(self, question, df):\n",
    "        # Load the YAML file (assumes it's in the current directory)\n",
    "        file_path = \"field_description.yaml\"\n",
    "        yaml_file_path = os.path.join(os.getcwd(), file_path)\n",
    "        field_data = self.load_yaml(yaml_file_path)\n",
    "\n",
    "        # few_shots_prompt \n",
    "        file_path = \"few_shot_prompts.yaml\"\n",
    "        yaml_file_path = os.path.join(os.getcwd(), file_path)\n",
    "        few_shot_prompts_data = self.load_yaml(yaml_file_path)\n",
    "        few_shot_prompts_data_str = json.dumps(few_shot_prompts_data)\n",
    "\n",
    "        # connector\n",
    "        df_name = \"summary\"\n",
    "\n",
    "        connector = PandasConnector({\"original_df\": df}\n",
    "                            , name=f\"{df_name}\"\n",
    "                            , description=field_data.get(f\"{df_name}\").get(\"table_description\", \" \")\n",
    "                            , field_descriptions=field_data.get(f\"{df_name}\").get(\"field_description\", \" \"))\n",
    "\n",
    "        # Initialize LLM\n",
    "        llm = OpenAI(model=self.llm_model, temperature=self.temperature, seed=10)\n",
    "\n",
    "        # Create the agent\n",
    "        agent = Agent(\n",
    "            connector,\n",
    "            memory_size=self.memory_size,\n",
    "            description=f\"\"\"\n",
    "You are a 25+ year experienced `Pro Python & Pandas Expert`. You specialize in developing precise and optimized DataFrame generation code.\n",
    "\n",
    "Primary Objective: Assist non-technical users by providing an aggregated DataFrame at the **most relevant level of granularity** needed to answer the question accurately, while avoiding excessively long DataFrames.\n",
    "\n",
    "Key Guidelines:\n",
    "- Use the **latest** `Year_Month` data by default, unless otherwise specified. Mention the data period used in the output.\n",
    "- If no relevant data is found within the documents, respond with `\"no_data\"`.\n",
    "\n",
    "Code Generation Rules:\n",
    "1. Return only concise DataFrames:\n",
    "   - Ensure aggregation is as concise as necessary to answer the question without producing unnecessary long DataFrames.\n",
    "   - STRICTLY avoid generating any charts or visualizations.\n",
    "2. Sanity checks & Error Handling:\n",
    "   - Perform internal error handling and data validations before presenting the output to ensure correctness.\n",
    "3. When fetching maximum or minimum values:\n",
    "   - If multiple items have the same value, include all matching items in the response, ensuring the result is comprehensive but concise.\n",
    "\n",
    "Few-shot Examples:\n",
    "{{few_shot_prompts_data_str}}\n",
    "\"\"\",\n",
    "            config={\n",
    "                \"llm\": llm,\n",
    "                \"open_charts\": False,\n",
    "                \"save_charts\": False,\n",
    "                \"verbose\": False,\n",
    "                \"save_logs\": False,\n",
    "                \"response_parser\": GenieResponse,\n",
    "                \"max_retries\":1\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Rephrase query and ask clarification questions\n",
    "        rephrased_prompt = agent.rephrase_query(question)\n",
    "        # clarification_questions = agent.clarification_questions(str(question))\n",
    "\n",
    "        # Run the initial prompt and rephrased prompt concurrently\n",
    "        response_dict = dict()\n",
    "\n",
    "        response_dict[question] = self.run_chat(question + \" \" + rephrased_prompt, agent)\n",
    "\n",
    "        # Run clarification questions concurrently\n",
    "        # clarifications_responses = [self.run_chat(que, agent) for que in clarification_questions]\n",
    "\n",
    "        # for que, resp in zip(clarification_questions, clarifications_responses):\n",
    "        #     response_dict[que] = resp\n",
    "\n",
    "        return response_dict\n",
    "\n",
    "    @component.output_types(replies=str)\n",
    "    def run(self, user_asked_query: str, df_list: List, **kwargs):\n",
    "        data_context_dict = self.run_agent(user_asked_query, df_list[0])\n",
    "        if \"Error\" in data_context_dict.keys():\n",
    "            return {\"replies\": \"go_to_rag\"}\n",
    "        return {\"replies\": json.dumps(data_context_dict)}\n",
    "    \n",
    "\n",
    "# Here are few shot examples to reference and modify onto whenever you need to generate code :\n",
    "#                 {{few_shot_prompts_data_str}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_context_prompt = \"\"\"\n",
    "Supporting dataframes:\n",
    "{{documents}}\n",
    "\n",
    "Conversation history (for context and understanding):\n",
    "{% for memory in memories %}\n",
    "    {{ memory.content }}\n",
    "{% endfor %}\n",
    "\n",
    "Use the conversation history to enhance your understanding of the user's question, but prioritize the provided dataframes to generate an accurate, logical, and inference-driven answer. If the dataframes do not directly provide an answer, say \"no_data\" and ask for clarifications if necessary.\n",
    "\n",
    "### Instructions:\n",
    "1. **Intelligent analysis over pasting**: Avoid simply pasting the DataFrame. Analyze the data, infer patterns, trends, and insights to address the user‚Äôs question directly.\n",
    "2. Respond as a **Pro Business Analyst**, delivering a **well-structured**, **visually enhanced** response. Use bulleted lists, tables, and visual aids like headers where necessary. Dynamically add **emojis** to represent trends, key points, or important facts (e.g., üìà for growth, üìù for action items).\n",
    "3. Use facts, numbers (rounded to 2 decimal places), and key insights. Ensure the response draws conclusions based on the data provided, and where possible, provide **projections** based on current trends.\n",
    "4. **Visual Outputs**: Feel free to create relevant markdown outputs, such as:\n",
    "    - **Tables** for data comparisons.\n",
    "    - **Bullet points** for key insights.\n",
    "    - **Headers** to break down sections visually.\n",
    "    - **Block quotes** for highlighting critical insights or recommendations.\n",
    "5. For numerical data with more than 2 rows, format it as a **pivot table** or another visual aid, summarizing insights.\n",
    "6. **Dynamic Sentence Structure**: Ensure responses are engaging by varying between bullet points, sentences, and block quotes to avoid repetition.\n",
    "7. **Intelligently insert relevant emojis** to enrich the response, based on the context of the content:\n",
    "    - üìä for key metrics or numerical data.\n",
    "    - üìà for growth trends and üìâ for declines.\n",
    "    - üîë for key insights.\n",
    "    - üìù for action steps and recommendations.\n",
    "    - üí° for ideas or future projections.\n",
    "8. Dynamically understand the current question, adapt the answer accordingly, and ensure the emojis and structure reflect the **tone** of the data (e.g., positive trends = üìà, record profits = üèÜ).\n",
    "9. **Forward-looking Analysis**: Where possible, project trends or offer recommendations for the future based on the current data.\n",
    "\n",
    "---\n",
    "\n",
    "### üìä **Key Metrics**:\n",
    "\n",
    "| **Metric**        | **Value**    | **Insights**                      |\n",
    "|-------------------|--------------|-----------------------------------|\n",
    "| **Sales Growth**  | **15.23%**   | Growth in the last quarter üìà      |\n",
    "| **Profit**        | **$1.2M**    | Highest in the last 3 years üèÜ     |\n",
    "| **Expenses**      | **$0.75M**   | Operational cost reduction üßæ      |\n",
    "\n",
    "---\n",
    "\n",
    "### üîë **Key Insights**:\n",
    "> **Sales Growth** has been **consistent** over the last quarter, driven by increased demand.\n",
    "> **Profit** is at a **record high** due to **cost optimization**. üìà  \n",
    "> **Expenses** remain **stable**, with room for further reductions ‚öñÔ∏è.\n",
    "\n",
    "---\n",
    "\n",
    "### üìù **Actionable Steps**:\n",
    "1. Focus on **regions with high sales growth** üìç for further opportunities.\n",
    "2. Analyze **expense categories** üßæ to identify areas for additional savings.\n",
    "3. Implement **data-driven decisions** üí° for the next financial quarter.\n",
    "\n",
    "---\n",
    "\n",
    "### **Next Steps**:\n",
    "- Segment data by:\n",
    "  - **Region** üó∫Ô∏è\n",
    "  - **Product Category** üì¶\n",
    "  - **Customer Segments** üë•\n",
    "\n",
    "**Next Actions**: Use these segments to create targeted strategies for the next fiscal year üíº.\n",
    "\n",
    "\\nUser's Question: {{query}}\n",
    "\\nAnswer:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Pipeline Selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector_prompt = \"\"\"\n",
    "Based on the conversation history, user question, and the table structure with column descriptions, decide which ONE of the following should be used for the next step of analysis:\n",
    "\n",
    "1. **DATAFRAME Pipeline**: Choose this if:\n",
    "   - The question involves numerical columns, operations, or comparisons (e.g., sorting, filtering, grouping by a column).\n",
    "   - The user is asking for a straightforward statistical or numerical operation that can be executed directly on the dataset.\n",
    "   - Any temporal component (e.g., date, month, year) is mentioned in the user question or the analysis involves time-based data.\n",
    "   - If analysis is required through temporal components such as trends over time, comparisons across different time periods, or sorting by date.\n",
    "\n",
    "2. **RAG Pipeline**: Choose this if:\n",
    "   - The question involves analyzing unstructured textual data embedded in the table (e.g., free-text fields, long descriptions).\n",
    "   - The user is asking for insights or analysis that require processing textual data not tied to specific numerical operations.\n",
    "   - The query asks for a broader, text-based analysis that cannot be executed using structured data operations.\n",
    "\n",
    "3. **GENERIC Response**: Choose this ONLY if:\n",
    "   - The user input is a greeting, irrelevant comment, or text that does not require analytical processing.\n",
    "   - The query does not involve any meaningful analysis and can be answered with a simple, generic response.\n",
    "\n",
    "### Decision Rules:\n",
    "- If the question involves **temporal data** (e.g., date, month, year), or asks for **quantified analysis** like comparisons, trends, or operations involving structured data, choose the `DATAFRAME Pipeline`.\n",
    "- When both **textual** and **numerical** elements are present, prioritize the pipeline that aligns best with the core requirement of the question. If the core focus is on numerical or temporal operations, select `DATAFRAME Pipeline`.\n",
    "- **Prioritize the `DATAFRAME Pipeline`** when any analysis involves numerical or temporal data, even if general text is included.\n",
    "- Only choose the `RAG Pipeline` if the question centers around unstructured textual analysis that cannot be handled by numerical or temporal operations.\n",
    "- **Fallback to the `DATAFRAME Pipeline`** if there is ambiguity between text and data analysis.\n",
    "- Use the `GENERIC Response` for clearly irrelevant questions or those unrelated to the available data.\n",
    "\n",
    "### Table Structure and Column Descriptions:\n",
    "    ```\n",
    "    Table descriptions:\n",
    "    {{table_description}}\n",
    "\n",
    "    Fields/Columns description:\n",
    "    {{field_descriptions}}\n",
    "    ```\n",
    "\n",
    "Carefully analyze the conversation history, table structure, and the user question. Choose ONLY ONE option strictly from: `RAG Pipeline`, `DATAFRAME Pipeline`, or `GENERIC Response`.\n",
    "\n",
    "Conversation history:\n",
    "{% for memory in memories %}\n",
    "    {{ memory.content }}\n",
    "{% endfor %}\n",
    "\n",
    "User Question:\n",
    "{{query}}\n",
    "\n",
    "Return only one: `RAG Pipeline`, `DATAFRAME Pipeline`, or `GENERIC Response`.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "routes = [\n",
    "    {\n",
    "        \"condition\": \"{{'rag' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_rag\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'dataframe' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_df\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'generic' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_generic\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'generic' not in replies[0]|lower}}\",\n",
    "        \"output\": \"{{replies[0]}}\",\n",
    "        \"output_name\": \"answer\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack import Pipeline\n",
    "# from haystack.dataclasses import ChatMessage\n",
    "\n",
    "# from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "# from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "# from haystack.components.routers import ConditionalRouter\n",
    "\n",
    "# router = ConditionalRouter(routes=routes)\n",
    "\n",
    "\n",
    "# demo_pipe = Pipeline()\n",
    "# demo_pipe.add_component(\"selector_prompt\", ChatPromptBuilder(template=[ChatMessage.from_user(selector_prompt)]))\n",
    "# demo_pipe.add_component(\"llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0, 'seed':101, 'n':1 }))\n",
    "# demo_pipe.add_component(\"router\", ConditionalRouter(routes=routes))\n",
    "\n",
    "# demo_pipe.connect(\"selector_prompt.prompt\", \"llm.messages\")\n",
    "# demo_pipe.connect(\"llm.replies\", \"router.replies\")\n",
    "\n",
    "# while True:\n",
    "#     question = input(\"Enter your question or Q to exit.\\nüßë \")\n",
    "#     if question==\"Q\":\n",
    "#         break\n",
    "#     print(question)\n",
    "#     response = demo_pipe.run( data={\n",
    "#         \"selector_prompt\": {\"query\": question, \"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "#         \"router\": {\"query\": question},\n",
    "#         })\n",
    "#     print(response)\n",
    "#     # print(response[\"llm\"][\"replies\"][0].content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Generic Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_response_prompt = \"\"\"You are Genie Bot, an advanced and friendly analytical assistant developed by i-Genie.\n",
    "Given the conversation history, and mainly Based on the user's message, respond appropriately, whether it's a greeting, generic question, or analytical inquiry.\n",
    "Adjust your tone according to the user's sentiment, whether positive, neutral, or negative, while always maintaining professionalism.\n",
    "\n",
    "### Response Instructions:\n",
    "\n",
    "1. **If the user greets you**:\n",
    "   - Respond with enthusiasm: \"Hello! It's great to hear from you! üòä How can I assist you today with your data analysis?\"\n",
    "\n",
    "2. **If the user asks a generic or irrelevant question**:\n",
    "   - Attempt to answer based on general knowledge, but suggest the user ask more data-related questions for better insights.\n",
    "   - Politely guide the user with examples of business analysis questions, dynamically generated based on the provided data description.\n",
    "\n",
    "3. **If the user provides positive feedback**:\n",
    "   - Acknowledge their satisfaction: \"I'm glad that was helpful! üòä\"\n",
    "   - Then offer further analysis suggestions tailored to the data: \"Would you like to explore more detailed insights based on the current dataset?\"\n",
    "\n",
    "4. **If the user responds negatively**:\n",
    "   - Stay calm and acknowledge potential limitations: \"I understand this might not be what you expected, and I‚Äôm still learning. Sometimes, I may not be perfect.\"\n",
    "   - Encourage collaboration: \"Let‚Äôs explore further together. Based on the dataset, you can ask specific questions that may help refine the analysis.\"\n",
    "   - Reassure them: \"Your feedback helps me improve, and we can work together to find the right insights.\"\n",
    "\n",
    "5. **If the user asks a data-related question**:\n",
    "   - Provide the analysis as requested, and then offer additional suggestions for further exploration, based on the dataset.\n",
    "\n",
    "6. **MUST suggest dynamic follow-up questions** based on the table and data description, without revealing schema details. Use the dataset to generate relevant, meaningful questions dynamically to keep the conversation engaging, but ensure that they are answerable from dataset.\n",
    "\n",
    "````\n",
    "### Underlying dataset Reference:\n",
    "Below is the general description of the dataset, which should guide your suggestions:\n",
    "    Table descriptions:\n",
    "    {{table_description}}\n",
    "\n",
    "    Fields/Columns description:\n",
    "    {{field_descriptions}}\n",
    "````\n",
    "\n",
    "    Conversation history:\n",
    "    {% for memory in memories %}\n",
    "        {{ memory.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "### Task:\n",
    "1. Judge the user's message and tone to respond accordingly, whether it's a greeting, irrelevant query, positive or negative feedback, or data-related question.\n",
    "2. Always suggest helpful business analysis question examples, crafted based on the data description & conversation history. You should be dynamically tailoring responses and follow-up questions based on the data description, coversation history and more importantly user question.\n",
    "3. While suggesting question examples, ensure they are not much complex yet insightful, and data oriented.\n",
    "4. Encourage deeper data exploration, keeping your tone friendly and professional, while guiding the user toward meaningful insights.\n",
    "\n",
    "Keep your tone friendly, professional, and energetic where appropriate! You are here to help the user make the most of the tool!\n",
    "\n",
    "\\n **User Message**: {{query}}\n",
    "\\n **Response**:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x737f24acb2c0>\n",
       "üöÖ Components\n",
       "  - s_prompt: ChatPromptBuilder\n",
       "  - s_lv: LengthValidator\n",
       "  - s_llm: OpenAIChatGenerator\n",
       "  - s_router: ConditionalRouter\n",
       "  - g_prompt: ChatPromptBuilder\n",
       "  - g_lv: LengthValidator\n",
       "  - g_llm: OpenAIChatGenerator\n",
       "  - dc_generator: DataContextGenerator\n",
       "  - dc_prompt_builder: ChatPromptBuilder\n",
       "  - dc_lv: LengthValidator\n",
       "  - dc_llm: OpenAIChatGenerator\n",
       "  - r_query_embedder: OpenAITextEmbedder\n",
       "  - r_retriever: InMemoryEmbeddingRetriever\n",
       "  - r_ranker: SentenceTransformersDiversityRanker\n",
       "  - r_batch_generator: BatchGenerator\n",
       "  - r_prompt_builder: ChatPromptBuilder\n",
       "  - r_lv: LengthValidator\n",
       "  - r_llm: OpenAIChatGenerator\n",
       "  - memory_retriever: ChatMessageRetriever\n",
       "  - memory_writer: ChatMessageWriter\n",
       "  - memory_joiner: BranchJoiner\n",
       "üõ§Ô∏è Connections\n",
       "  - s_prompt.prompt -> s_lv.prompt (List[ChatMessage])\n",
       "  - s_lv.prompt -> s_llm.messages (List[ChatMessage])\n",
       "  - s_llm.replies -> s_router.replies (List[ChatMessage])\n",
       "  - s_router.go_to_rag -> r_query_embedder.text (str)\n",
       "  - s_router.go_to_df -> dc_generator.user_asked_query (str)\n",
       "  - s_router.go_to_generic -> g_prompt.query (str)\n",
       "  - g_prompt.prompt -> g_lv.prompt (List[ChatMessage])\n",
       "  - g_lv.prompt -> g_llm.messages (List[ChatMessage])\n",
       "  - g_llm.replies -> memory_joiner.value (List[ChatMessage])\n",
       "  - dc_generator.replies -> dc_prompt_builder.documents (str)\n",
       "  - dc_prompt_builder.prompt -> dc_lv.prompt (List[ChatMessage])\n",
       "  - dc_lv.prompt -> dc_llm.messages (List[ChatMessage])\n",
       "  - dc_llm.replies -> memory_joiner.value (List[ChatMessage])\n",
       "  - r_query_embedder.embedding -> r_retriever.query_embedding (List[float])\n",
       "  - r_retriever.documents -> r_ranker.documents (List[Document])\n",
       "  - r_ranker.documents -> r_batch_generator.documents (List[Document])\n",
       "  - r_batch_generator.replies -> r_prompt_builder.documents (List)\n",
       "  - r_prompt_builder.prompt -> r_lv.prompt (List[ChatMessage])\n",
       "  - r_lv.prompt -> r_llm.messages (List[ChatMessage])\n",
       "  - r_llm.replies -> memory_joiner.value (List[ChatMessage])\n",
       "  - memory_retriever.messages -> s_prompt.memories (List[ChatMessage])\n",
       "  - memory_retriever.messages -> dc_prompt_builder.memories (List[ChatMessage])\n",
       "  - memory_retriever.messages -> r_prompt_builder.memories (List[ChatMessage])\n",
       "  - memory_retriever.messages -> g_prompt.memories (List[ChatMessage])\n",
       "  - memory_joiner.value -> memory_writer.messages (List[ChatMessage])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List\n",
    "from haystack import Pipeline\n",
    "from haystack.components.builders import ChatPromptBuilder, PromptBuilder\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.joiners import BranchJoiner\n",
    "from haystack.components.routers import ConditionalRouter\n",
    "\n",
    "from haystack_experimental.chat_message_stores.in_memory import InMemoryChatMessageStore\n",
    "from haystack_experimental.components.retrievers import ChatMessageRetriever\n",
    "from haystack_experimental.components.writers import ChatMessageWriter\n",
    "\n",
    "########################################\n",
    "##   CUSTOM COMPONENT INSTANTIATION   ##\n",
    "########################################\n",
    "\n",
    "# Memory Components Instantiation\n",
    "memory_store = InMemoryChatMessageStore()\n",
    "memory_retriever = ChatMessageRetriever(memory_store)\n",
    "memory_writer = ChatMessageWriter(memory_store)\n",
    "\n",
    "# Custom Components Instantiation\n",
    "batch_generator = BatchGenerator(question_prompt=map_prompt\n",
    "                                 , system_prompt=system_prompt\n",
    "                                 , table_description=table_description\n",
    "                                 , field_descriptions=field_descriptions\n",
    "                                 , batch_size=5)\n",
    "\n",
    "data_context_generator = DataContextGenerator()\n",
    "\n",
    "# Router Instantiation\n",
    "routes = [\n",
    "    {\n",
    "        \"condition\": \"{{'rag' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_rag\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'dataframe' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_df\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'generic' in replies[0]|lower}}\",\n",
    "        \"output\": \"{{query}}\",\n",
    "        \"output_name\": \"go_to_generic\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "    {\n",
    "        \"condition\": \"{{'generic' not in replies[0]|lower}}\",\n",
    "        \"output\": \"{{replies[0]}}\",\n",
    "        \"output_name\": \"go_to_rag\",\n",
    "        \"output_type\": str,\n",
    "    },\n",
    "]\n",
    "\n",
    "########################################\n",
    "##     PIPELINE DEFINITION            ##\n",
    "########################################\n",
    "\n",
    "pipeline = Pipeline()\n",
    "\n",
    "# components for Selected Routing\n",
    "pipeline.add_component(\"s_prompt\", ChatPromptBuilder(template=[ChatMessage.from_user(selector_prompt)]))\n",
    "pipeline.add_component(\"s_lv\", LengthValidator())\n",
    "pipeline.add_component(\"s_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0, 'seed':101, 'n':1, 'max_tokens':100 }))\n",
    "pipeline.add_component(\"s_router\", ConditionalRouter(routes=routes))\n",
    "\n",
    "# components for Generic Response\n",
    "pipeline.add_component(\"g_prompt\", ChatPromptBuilder(template=[ChatMessage.from_user(generic_response_prompt)]))\n",
    "pipeline.add_component(\"g_lv\", LengthValidator())\n",
    "pipeline.add_component(\"g_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0.5, 'seed':101, 'n':1, 'max_tokens':2000 }))\n",
    "\n",
    "# components for DataFrame Context\n",
    "pipeline.add_component(\"dc_generator\", data_context_generator)\n",
    "pipeline.add_component(\"dc_prompt_builder\", ChatPromptBuilder(template=[ChatMessage.from_user(data_context_prompt)]))\n",
    "pipeline.add_component(\"dc_lv\", LengthValidator())\n",
    "pipeline.add_component(\"dc_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0.3, 'seed':101, 'n':1 , 'max_tokens':2000}))\n",
    "\n",
    "# components for RAG\n",
    "pipeline.add_component(\"r_query_embedder\", OpenAITextEmbedder())\n",
    "pipeline.add_component(\"r_retriever\", InMemoryEmbeddingRetriever(document_store=document_store))\n",
    "pipeline.add_component(\"r_ranker\", SentenceTransformersDiversityRanker())\n",
    "pipeline.add_component(\"r_batch_generator\", batch_generator)\n",
    "pipeline.add_component(\"r_prompt_builder\", ChatPromptBuilder(template=[ChatMessage.from_user(reduce_prompt)]))\n",
    "pipeline.add_component(\"r_lv\", LengthValidator())\n",
    "pipeline.add_component(\"r_llm\", OpenAIChatGenerator(model=\"gpt-4o-mini\", generation_kwargs={'temperature':0.3, 'seed':101, 'n':1, 'max_tokens':2000 }))\n",
    "\n",
    "# components for memory\n",
    "pipeline.add_component(\"memory_retriever\", memory_retriever)\n",
    "pipeline.add_component(\"memory_writer\", memory_writer)\n",
    "pipeline.add_component(\"memory_joiner\", BranchJoiner(List[ChatMessage]))\n",
    "\n",
    "# connections for Selected routing\n",
    "pipeline.connect(\"s_prompt\", \"s_lv\")\n",
    "pipeline.connect(\"s_lv.prompt\", \"s_llm.messages\")\n",
    "pipeline.connect(\"s_llm.replies\", \"s_router.replies\")\n",
    "pipeline.connect(\"s_router.go_to_rag\", \"r_query_embedder.text\")\n",
    "pipeline.connect(\"s_router.go_to_df\", \"dc_generator.user_asked_query\")\n",
    "pipeline.connect(\"s_router.go_to_generic\", \"g_prompt.query\")\n",
    "\n",
    "# connection for Generic Response\n",
    "pipeline.connect(\"g_prompt\", \"g_lv\")\n",
    "pipeline.connect(\"g_lv.prompt\", \"g_llm.messages\")\n",
    "pipeline.connect(\"g_llm.replies\", \"memory_joiner\")\n",
    "\n",
    "# connections for RAG\n",
    "pipeline.connect(\"r_query_embedder.embedding\", \"r_retriever.query_embedding\")\n",
    "pipeline.connect(\"r_retriever.documents\", \"r_ranker.documents\")\n",
    "pipeline.connect(\"r_ranker.documents\", \"r_batch_generator.documents\")\n",
    "pipeline.connect(\"r_batch_generator.replies\", \"r_prompt_builder.documents\")\n",
    "pipeline.connect(\"r_prompt_builder\", \"r_lv\")\n",
    "pipeline.connect(\"r_lv.prompt\", \"r_llm.messages\")\n",
    "pipeline.connect(\"r_llm.replies\", \"memory_joiner\")\n",
    "\n",
    "# connections for DataFrame Context\n",
    "pipeline.connect(\"dc_generator.replies\", \"dc_prompt_builder.documents\")\n",
    "pipeline.connect(\"dc_prompt_builder\", \"dc_lv\")\n",
    "pipeline.connect(\"dc_lv.prompt\", \"dc_llm.messages\")\n",
    "pipeline.connect(\"dc_llm.replies\", \"memory_joiner\")\n",
    "\n",
    "# connections for memory\n",
    "pipeline.connect(\"memory_joiner\", \"memory_writer\")\n",
    "pipeline.connect(\"memory_retriever\", \"s_prompt.memories\")\n",
    "pipeline.connect(\"memory_retriever\", \"dc_prompt_builder.memories\")\n",
    "pipeline.connect(\"memory_retriever\", \"r_prompt_builder.memories\")\n",
    "pipeline.connect(\"memory_retriever\", \"g_prompt.memories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversational Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_message = ChatMessage.from_system(system_prompt)\n",
    "\n",
    "# while True:\n",
    "#     messages = [system_message,]\n",
    "#     question = input(\"Enter your question or Q to exit.\\nüßë \")\n",
    "#     if question==\"Q\":\n",
    "#         break\n",
    "\n",
    "#     res = pipeline.run(data={\"s_prompt\": {\"query\": question, \"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "#                              \"g_prompt\": {\"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "#                              \"s_router\": {\"query\": question},\n",
    "#                              \"dc_generator\": {\"df_list\":[df,]},\n",
    "#                              \"dc_prompt_builder\": {\"query\": question},\n",
    "#                              \"r_retriever\": {\"top_k\": 50},\n",
    "#                              \"r_batch_generator\": {\"user_asked_query\": question,},\n",
    "#                              \"r_prompt_builder\": {\"query\": question},\n",
    "#                              \"memory_joiner\": {\"value\": [ChatMessage.from_user(question)]}\n",
    "#                              },\n",
    "#                             include_outputs_from=['g_llm', 'dc_llm', 'r_llm', \"s_prompt\", \"s_router\",\n",
    "#                                                   \"dc_generator\", \"dc_prompt_builder\",\n",
    "#                                                   \"r_retriever\", \"r_batch_generator\", \"r_prompt_builder\", \"memory_retriever\"  ]\n",
    "#                         )\n",
    "\n",
    "#     llm_type = [key for key in ['g_llm', 'dc_llm', 'r_llm'] if key in res][0]\n",
    "#     assistant_resp = res[llm_type]['replies'][0]\n",
    "#     display(Markdown(f\"{assistant_resp.content}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res[\"memory_retriever\"][\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradio Bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [ChatMessage.from_system(system_prompt)]\n",
    "\n",
    "def chat(question, history):\n",
    "    messages.append(ChatMessage.from_user(question))\n",
    "    response = pipeline.run(data={\"s_prompt\": {\"query\": question, \"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "                             \"g_prompt\": {\"table_description\":table_description, \"field_descriptions\":field_descriptions},\n",
    "                             \"s_router\": {\"query\": question},\n",
    "                             \"dc_generator\": {\"df_list\":[df,]},\n",
    "                             \"dc_prompt_builder\": {\"query\": question},\n",
    "                             \"r_retriever\": {\"top_k\": 50},\n",
    "                             \"r_ranker\": {\"query\": question, \"top_k\": 20},\n",
    "                             \"r_batch_generator\": {\"user_asked_query\": question,},\n",
    "                             \"r_prompt_builder\": {\"query\": question},\n",
    "                             \"memory_joiner\": {\"value\": [ChatMessage.from_user(question)]}\n",
    "                             },\n",
    "                            include_outputs_from=['g_llm', 'dc_llm', 'r_llm', \"s_prompt\", \"s_router\",\n",
    "                                                  \"dc_generator\", \"dc_prompt_builder\",\n",
    "                                                  \"r_retriever\", \"r_batch_generator\", \"r_prompt_builder\",\n",
    "                                                   \"memory_retriever\" ]\n",
    "                        )\n",
    "    llm_type = [key for key in ['g_llm', 'dc_llm', 'r_llm'] if key in response][0]\n",
    "    assistant_resp = response[llm_type]['replies']\n",
    "    messages.extend(assistant_resp)\n",
    "\n",
    "    return assistant_resp[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caching examples at: '/workspaces/g_bot/gradio_cached_examples/15'\n",
      "Caching example 1/2\n",
      "Caching example 2/2\n",
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a47c2ee0c5c4222281.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a47c2ee0c5c4222281.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Batch Size 4 from 20 documents\n"
     ]
    }
   ],
   "source": [
    "demo = gr.ChatInterface(\n",
    "    fn=chat,\n",
    "    examples=[\n",
    "        \"What is the overall score?\",\n",
    "        \"How are my brands performing?\",\n",
    "    ],\n",
    "    title=\"Brand Pulse Bot\",\n",
    "    cache_examples=True,\n",
    "    # analytics_enabled=True,\n",
    "    show_progress='full',\n",
    "    fill_height=True,\n",
    "\n",
    ")\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
